cv <- xgb.cv(variable_matrix, label = labelvec,
params=list(
objective="binary:logistic"
),
nfold=5,
nrounds=100,
print_every_n=10,
metrics="logloss")
evalframe <- as.data.frame(cv$evaluation_log)
NROUNDS <- which.min(evalframe$test_logloss_mean)
model <- xgboost(data=variable_matrix, label=labelvec,
params=list(
objective="binary:logistic"
),
nrounds=NROUNDS,
verbose=FALSE)
model
}
# define AUC function for XGboost
calculate_auc <- function(model, mat_data, actual_labels, dataset_name) {
pred_prob <- predict(model, newdata = mat_data)
auc_val <- calcAUC(pred_prob, actual_labels)
cat("AUC for", dataset_name, ": ", auc_val, "\n")
return(auc_val)
}
# train the model
model1 <- xgbooost(matTrain1, dTrain1$income_class)
model2 <- xgbooost(matTrain2, dTrain2$income_class)
model3 <- xgbooost(matTrain3, dTrain3$income_class)
# calculate the AUC with the function defined above
auc_train1 <- calculate_auc(model1, matTrain1, dTrain1$income_class, "Training 1")
auc_cal1 <- calculate_auc(model1, matCal1, dCal1$income_class, "Calibration 1")
auc_test1 <- calculate_auc(model1, matTest1, dTest1$income_class, "Test 1")
auc_train2 <- calculate_auc(model2, matTrain2, dTrain2$income_class, "Training 2")
auc_cal2 <- calculate_auc(model2, matCal2, dCal2$income_class, "Calibration 2")
auc_test2 <- calculate_auc(model2, matTest2, dTest2$income_class, "Test 2")
auc_train3 <- calculate_auc(model3, matTrain3, dTrain3$income_class, "Training 3")
auc_cal3 <- calculate_auc(model3, matCal3, dCal3$income_class, "Calibration 3")
auc_test3 <- calculate_auc(model3, matTest3, dTest3$income_class, "Test 3")
# define the function for XGBoost performance
performanceMeasures <- function(ytrue, ypred_prob, model.name = "model", threshold=0.5) {
ypred <- ifelse(ypred_prob > threshold, 1, 0)
# compute the normalised deviance
dev.norm <- -2 * logLikelihood(ytrue, ypred_prob)/length(ypred_prob)
# compute the confusion matrix
cmat <- table(actual = ytrue, predicted = ypred)
accuracy <- sum(diag(cmat)) / sum(cmat)
precision <- cmat[2, 2] / sum(cmat[, 2])
recall <- cmat[2, 2] / sum(cmat[2, ])
f1 <- 2 * precision * recall / (precision + recall)
data.frame(model = model.name,
precision = precision, recall = recall, f1 = f1, dev.norm = dev.norm)
}
panderOpt <- function(){
panderOptions("plain.ascii", TRUE)
panderOptions("keep.trailing.zeros", TRUE)
panderOptions("table.style", "simple")
}
# pretty_perf_table
pretty_perf_table <- function(model, xtrain, ytrain, xtest, ytest, threshold=0.5) {
# Option setting for Pander
panderOpt()
perf_justify <- "lrrrr"
# call the predict() function to do the predictions
pred_train <- predict(model, newdata=xtrain)
pred_test <- predict(model, newdata=xtest)
# comparing performance on training vs. test
trainperf_df <- performanceMeasures(ytrain, pred_train, model.name="training", threshold=threshold)
testperf_df <- performanceMeasures(ytest, pred_test, model.name="test", threshold=threshold)
# combine the two performance data frames using rbind()
perftable <- rbind(trainperf_df, testperf_df)
pandoc.table(perftable, justify = perf_justify)
}
pretty_perf_table(model1, matTrain1, dTrain_int1$income_class, matTest1, dTest_int1$income_class)
pretty_perf_table(model2, matTrain2, dTrain_int2$income_class, matTest2, dTest_int2$income_class)
pretty_perf_table(model3, matTrain3, dTrain_int3$income_class, matTest3, dTest_int3$income_class)
explainer <- function(model, variable_matrix) {
explainer <- lime(as.data.frame(variable_matrix), model)
explanation <- explain(as.data.frame(variable_matrix)[1:4,], explainer, n_labels = 1, n_features = 5)
explanation
}
explanation2 <- explainer(model2, matTrain2)
print(explanation2)
plot_features(explanation2)
# use the xg.importance to show the distribution of feature weight
importances <- xgb.importance(model = model2)
xgb.plot.importance(importances)
#refresh the data
dTrain1 <- dTrain %>%
select(all_of(c(selVars, outcome)))
dTrain2 <- dTrain %>%
select(all_of(c(selVar2, outcome)))
dTrain3 <- dTrain %>%
select(all_of(c(selVar3, outcome)))
dTest1 <- dTest %>%
select(all_of(c(selVars, outcome)))
dTest2 <- dTest %>%
select(all_of(c(selVar2, outcome)))
dTest3 <- dTest %>%
select(all_of(c(selVar3, outcome)))
dCal1 <- dCal %>%
select(all_of(c(selVars, outcome)))
dCal2 <- dCal %>%
select(all_of(c(selVar2, outcome)))
dCal3 <- dCal %>%
select(all_of(c(selVar3, outcome)))
#logistic regression
f <- paste(outcome,'~ ', paste(selVars, collapse=' + '), sep='')
cat(f)
cat("Feature dimension = ", length(selVars))
gmodel1 <- glm(as.formula(f), data=dTrain1, family=binomial(link='logit'))
print(calcAUC(predict(gmodel1, newdata=dTrain1), dTrain1[,outcome] ))
print(calcAUC(predict(gmodel1, newdata=dTest1), dTest1[,outcome] ))
print(calcAUC(predict(gmodel1, newdata=dCal1), dCal1[,outcome] ))
f <- paste(outcome,'~ ', paste(selVar2, collapse=' + '), sep='')
cat(f)
cat("Feature dimension = ", length(selVar2))
gmodel2 <- glm(as.formula(f), data=dTrain2, family=binomial(link='logit'))
#AUC Calculation
print(calcAUC(predict(gmodel2, newdata=dTrain2), dTrain2[,outcome] ))
print(calcAUC(predict(gmodel2, newdata=dTest2), dTest2[,outcome] ))
print(calcAUC(predict(gmodel2, newdata=dCal2), dCal2[,outcome] ))
f <- paste(outcome,'~ ', paste(selVar3, collapse=' + '), sep='')
cat(f)
cat("Feature dimension = ", length(selVar3))
gmodel3 <- glm(as.formula(f), data=dTrain3, family=binomial(link='logit'))
print(calcAUC(predict(gmodel3, newdata=dTrain3), dTrain3[,outcome] ))
print(calcAUC(predict(gmodel3, newdata=dTest3), dTest3[,outcome] ))
print(calcAUC(predict(gmodel3, newdata=dCal3), dCal3[,outcome] ))
#ROC plot
pred_gmodel3_roc <- predict(gmodel3, newdata=dTest3)
pred_gmodel2_roc <- predict(gmodel2, newdata=dTest2)
pred_gmodel1_roc <- predict(gmodel1, newdata=dTest1)
plot_roc(pred_gmodel1_roc, dTest1[[outcome]],
pred_gmodel2_roc, dTest2[[outcome]],
pred_gmodel3_roc, dTest3[[outcome]])
#performance matrix
pretty_perf_table(gmodel1, dTrain1[selVars], dTrain1[,outcome]==pos, dTest1[selVars], dTest1[,outcome]==pos)
pretty_perf_table(gmodel2, dTrain2[selVar2], dTrain2[,outcome]==pos, dTest2[selVar2], dTest2[,outcome]==pos)
pretty_perf_table(gmodel3, dTrain3[selVar3], dTrain3[,outcome]==pos, dTest3[selVar3], dTest3[,outcome]==pos)
#caret glm (instead of original one)
# Function to calculate AUC using ROCR
calcAUC_ROCR <- function(preds, truth) {
pred_obj <- prediction(preds, truth)
perf_obj <- performance(pred_obj, measure = "auc")
return(perf_obj@y.values[[1]])
}
# Model 1
f1 <- as.formula(paste(outcome,'~ ', paste(selVars, collapse=' + ')))
cat(deparse(f1))
cat("Feature dimension = ", length(selVars))
gmodel1_caret <- train(f1, data=dTrain1, method="glm", family=binomial(link='logit'))
print(calcAUC_ROCR(predict(gmodel1_caret, dTrain1), dTrain1[,outcome]))
print(calcAUC_ROCR(predict(gmodel1_caret, dTest1), dTest1[,outcome]))
print(calcAUC_ROCR(predict(gmodel1_caret, dCal1), dCal1[,outcome]))
# Model 2
f2 <- as.formula(paste(outcome,'~ ', paste(selVar2, collapse=' + ')))
cat(deparse(f2))
cat("Feature dimension = ", length(selVar2))
gmodel2_caret <- train(f2, data=dTrain2, method="glm", family=binomial(link='logit'))
print(calcAUC_ROCR(predict(gmodel2_caret, dTrain2), dTrain2[,outcome]))
print(calcAUC_ROCR(predict(gmodel2_caret, dTest2), dTest2[,outcome]))
print(calcAUC_ROCR(predict(gmodel2_caret, dCal2), dCal2[,outcome]))
# Model 3
f3 <- as.formula(paste(outcome,'~ ', paste(selVar3, collapse=' + ')))
cat(deparse(f3))
cat("Feature dimension = ", length(selVar3))
gmodel3_caret <- train(f3, data=dTrain3, method="glm", family=binomial(link='logit'))
print(calcAUC_ROCR(predict(gmodel3_caret, dTrain3), dTrain3[,outcome]))
print(calcAUC_ROCR(predict(gmodel3_caret, dTest3), dTest3[,outcome]))
print(calcAUC_ROCR(predict(gmodel3_caret, dCal3), dCal3[,outcome]))
#LIME conduct
explainer2 <- lime(dTrain2, gmodel2_caret, bin_continuous = TRUE)
observation2 <- dTest2[1:4, !(names(dTest2) %in% c("income_class"))]
explanation2 <- explain(observation2, explainer2, n_features = 7)
print(explanation2)
plot_features(explanation2)
vip_plot <- vip(gmodel2_caret, num_features = 13)
print(vip_plot)
#Performance Matrix and visualization
pretty_perf_table <- function(model, xtrain, ytrain, xtest, ytest, threshold=0.5) {
# Option setting for Pander
panderOpt()
perf_justify <- "lrrrr"
# call the predict() function to do the predictions
pred_train <- predict(model, newdata=xtrain)
pred_test <- predict(model, newdata=xtest)
# comparing performance on training vs. test
trainperf_df <- performanceMeasures( ytrain, pred_train, model.name="training", threshold=threshold)
testperf_df <- performanceMeasures( ytest, pred_test, model.name="test", threshold=threshold)
# combine the two performance data frames using rbind()
perftable <- rbind(trainperf_df, testperf_df)
return(perftable)
}
#LOGISTIC
perf1 <- pretty_perf_table(gmodel3, dTrain3[selVar3], dTrain3[,outcome]==pos, dTest3[selVar3], dTest3[,outcome]==pos)
#DECISION TREE
perf2 <- pretty_perf_table(t2model, dTrain2[selVar2], dTrain2[,outcome]==pos, dTest2[selVar2], dTest2[,outcome]==pos)
#XGBOOST
perf3 <- pretty_perf_table(model2, matTrain2, dTrain2$income_class, matTest2, dTest2$income_class)
perf1$model_type <- "Logistic Regression #3"
perf2$model_type <- "Decision Tree #2"
perf3$model_type <- "XGBoost #2"
perf1_test <- subset(perf1, model == "test")
perf2_test <- subset(perf2, model == "test")
perf3_test <- subset(perf3, model == "test")
all_perf_test <- rbind(perf1_test, perf2_test, perf3_test)
all_perf_long <- gather(all_perf_test, metric, value, -model_type, -model)
ggplot(all_perf_long, aes(x=model_type, y=value, color=model_type)) +
geom_point(size=4, position=position_dodge(0.6)) +
facet_wrap(~metric, scales="free_y", ncol=1) +
labs(title="Test Data Performance", y="Value") +
theme_minimal() +
theme(legend.position="right")
#ROC plot
roc_decision_tree <- roc(dTest3[,outcome], predict(t3model, newdata=dTest3))
roc_xgboost2 <- roc(dTest2$income_class, predict(model2, newdata=matTest2))
roc_logistic_regression <- roc(dTest2[,outcome], as.numeric(predict(gmodel2_caret, dTest2)))
ggroc(list(Decision_Tree_3=roc_decision_tree, XGBoost_2=roc_xgboost2, LogisticRegression_2=roc_logistic_regression))
#set a new data set for clustering, extracvt the valid vairbales from the processed df
df.cluster <- df[,c(catvars,numvars)]
#group the variables into numeric and category based on the type
cluster.catvars <- colnames(df.cluster)[sapply(df.cluster[, colnames(df.cluster)], class) %in%
c('factor', 'character')]
cluster.numvars <- colnames(df.cluster)[sapply(df.cluster[, colnames(df.cluster)], class) %in%
c('numeric', 'integer')]
#check the varibales
cluster.catvars
cluster.numvars
#convert category variables to dummy
dummy_vars <- model.matrix(~ . - 1, data = df.cluster[, cluster.catvars])
df.cluster <- df.cluster[, -which(names(df.cluster) %in% cluster.catvars)]
df.cluster <- cbind(df.cluster, dummy_vars)
colnames(df.cluster) <- gsub("(.*)\\((.*)\\)", "\\2_\\1", colnames(df.cluster))
#delete all cate variables
df.cluster$cluster.catvars <- NULL
#Scale all of the numeric columns in df.clustering
vars.to.use <- colnames(df.cluster)
scaled_df <- scale(df.cluster[,vars.to.use])
#Check the centre and sd of the vriables after scaling
attr(scaled_df, "scaled:center")
attr(scaled_df, "scaled:scale")
#calculate the Euclidean distance of data points first
(d <- dist(scaled_df, method="euclidean"))
#set the method of minimise the distance
pfit <- hclust(d, method="ward.D2")
#define the fucntion of distance square
sqr_euDist <- function(x,y) {
sum((x - y)^2)}
#define function of WSS, WSS total, TSS
wss <- function(clustermat) {
c0 <- colMeans(clustermat)
sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} )) }
wss_total <- function(scaled_df, labels) {
wss.sum <- 0
k <- length(unique(labels))
for (i in 1:k)
wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
wss.sum
}
tss <- function(scaled_df) {
wss(scaled_df) }
#define function of CH index
CH_index <- function(scaled_df, kmax, method = "kmeans") {
if (!(method %in% c("kmeans", "hclust")))
stop("method must be one of c('kmeans', 'hclust')")
npts <- nrow(scaled_df)
wss.value <- numeric(kmax)
wss.value[1] <- wss(scaled_df)
if (method == "kmeans") {
for (k in 2:kmax) {
clustering <- kmeans(scaled_df, k, nstart = 10, iter.max = 100)
wss.value[k] <- clustering$tot.withinss
}
} else {
d <- dist(scaled_df, method = "euclidean")
pfit <- hclust(d, method = "ward.D2")
for (k in 2:kmax) {
labels <- cutree(pfit, k = k)
wss.value[k] <- wss_total(scaled_df, labels)
}
}
bss.value <- tss(scaled_df) - wss.value
B <- bss.value / (0:(kmax-1))
W <- wss.value / (npts - 1:kmax)
data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
# calculate and plot total WSS and CH index
crit.df <- CH_index(scaled_df, 50, method="hclust")
#plot the total WSS
fig1 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") + geom_point() + geom_line(colour="blue") + scale_x_continuous(breaks=1:50, labels=1:50) + theme(text=element_text(size=10))+labs(title = "Total WSS Plot")
#plot the CH index
fig2 <- ggplot(crit.df, aes(x=k, y=CH_index)) + geom_point() + geom_line(colour="red") + scale_x_continuous(breaks=1:50, labels=1:50) + labs(y="CH index") + theme(text=element_text(size=10))+ labs(title = "CH index Plot")
fig1
fig2
#set the k as k=21
groups <- cutree(pfit, k=21)
print_clusters <- function(df, groups, cols_to_print) { Ngroups <- max(groups)
for (i in 1:Ngroups) {
print(paste("cluster", i))
print(df[groups == i, cols_to_print]) }
}
#check the cluster of 21 groups, it might hard to show in the html document
print_clusters(df.cluster, groups)
#to visualise the cluster, the dimension of the variables will be reduced to 2 (named principle component)
princ <- prcomp(scaled_df) # Calculate the principal components of scaled_df
nComp <- 2
project2D <- as.data.frame(predict(princ, newdata=scaled_df)[,1:nComp])
hclust.project2D <- cbind(project2D, cluster=as.factor(groups))
# finding the convex hull
find_convex_hull <- function(proj2Ddf, groups) {
do.call(rbind,
lapply(unique(groups),
FUN = function(c) {
f <- subset(proj2Ddf, cluster==c);
f[chull(f),]
}
)
)
}
hclust.hull <- find_convex_hull(hclust.project2D, groups)
#plot the clustering with k=21
ggplot(hclust.project2D, aes(x=PC1, y=PC2)) +
geom_point(aes(shape=cluster, color=cluster)) +
geom_text(aes(label= '', color=cluster), hjust=0, vjust=1, size=3) +
geom_polygon(data=hclust.hull, aes(group=cluster, fill=as.factor(cluster)),
alpha=0.4, linetype=0) + theme(text=element_text(size=10))+
labs(title = "H-Clustering Plot (K=21)")
#prepare the data frame for K mean clustering
df.kmean <- df.cluster[,c(cluster.numvars)]
#set the hyper parameter as 3, set run time as 100, and random starts as 100
kbest.p <- 3
kmClusters <- kmeans(df.kmean, kbest.p, nstart=100, iter.max=100)
#check the K centre and size
kmClusters$centers
kmClusters$size
#have a glance on the 3 groups
#As it is hard to show in HTML, just keep the code here
groups <- kmClusters$cluster
print_clusters(df.kmean, groups)
#run to pick the best K by CH index
kmClustering.ch <- kmeansruns(df.kmean, krange=1:50, criterion="ch")
kmClustering.ch$bestk
#run to pick the best K by Average silhouette width (AWS)
kmClustering.asw <- kmeansruns(df.kmean, krange=1:50, criterion="asw")
kmClustering.asw$bestk
#draw plots for two methods
kmCritframe <- data.frame(k=1:50, ch=kmClustering.ch$crit,
asw=kmClustering.asw$crit)
fig1 <- ggplot(kmCritframe, aes(x=k, y=ch)) +
geom_point() +
geom_line(colour="red") +
scale_x_continuous(breaks=1:50, labels=1:50) +
labs(y="CH index", title="CH index Plot") + theme(text=element_text(size=10))
fig2 <- ggplot(kmCritframe, aes(x=k, y=asw)) + geom_point() +
geom_line(colour="blue") +
scale_x_continuous(breaks=1:50, labels=1:50) +
labs(y="ASW", title="ASW Plot") +
theme(text=element_text(size=10))
fig1
fig2
fig <- c()
kvalues <- c(2,10,16)
for (k in kvalues) {
groups <- kmeans(scaled_df, k, nstart=100, iter.max=100)$cluster
kmclust.project2D <- cbind(project2D, cluster=as.factor(groups))
kmclust.hull <- find_convex_hull(kmclust.project2D, groups)
assign(paste0("fig", k),
ggplot(kmclust.project2D, aes(x=PC1, y=PC2)) + geom_point(aes(shape=cluster, color=cluster)) + geom_polygon(data=kmclust.hull, aes(group=cluster, fill=cluster),
alpha=0.4, linetype=0) + labs(title = sprintf("K mean Clustering: k = %d", k)) +
theme(legend.position="none", text=element_text(size=10))
) }
fig2
fig10
fig16
kbest.p <- 2
kmClusters <- kmeans(df.kmean, kbest.p, nstart=100, iter.max=100)
groups <- kmClusters$cluster
#check the different clusters. It is hard to show in HTML, so will only show the code
print_clusters(df.kmean, groups)
library(shiny)
library(ggplot2)
library(dplyr)
library(ggthemes)
library(ROCit)
library(pROC)
ui <- navbarPage(
title = "CITS4009 Project 2",
h5("Mingyu LIAN: 24046428 & Chensu YANG: 24035732"),
tabPanel(
"Single Variable Model Evaluation",
h3("ROC Plot for Single Variables", style = "color:Darkred"),
p("The ROC Plot shows the performance of each single variable model. There are 9 models in total will be compared, each of the model have its corresponding single variable. The closer the curve is to the top left corner, the better the model is."),
p("There is also a red point line shows the null model performance, which can be treated as the worst model. Normally, each of the single variable model should have a better performance than the null model, which means the curve should be above the red point line.
"),
h5("Please use the multi-select box to choose the variables you want to plot.",style = "color:grey"),
sidebarLayout(
sidebarPanel(
checkboxGroupInput("selected_columns", "Choose Variable to Plot:",
choices = c("region", "lst mth view", "uploads", "lst mth subscribe","subscribers","views","Vtreat Missing","numeric date","category"),
selected = c("region","lst mth view", "uploads", "lst mth subscribe")) ,
p('__________________________________'),
h4("Conclusion"),
p("From the observations, 4 variables: region, last month view, last month subcribe have notable better performance than other variables."),
tags$a(href="https://youtu.be/uQKxh1xiHzU?si=vuo7dlFLjMbL8sqK", "Click Here to Watch the Guide Video!")),
mainPanel(
plotOutput("ROC")
)
)
),
tabPanel(
"Multi-Variable Models Evaluation",
h3("Model Indicators Comparison", style = "color:Darkred"),
p("This comparison chart compares the peformance of 3 classifiers with its best variables combination respectively on test set. They are: Decision Tree Classifier with Variable Combination 2,
Logistic Regression Classifier with Variable Combination 3 and XGBoost Classifier with Variable Combination 2.Besides, There are 4 common used indicators in the charts,
they are: dev.norm, f1, precision and recall.
The higher the indicator value is, the better the model is."),
p("You can compare the performance of the 3 models by choosing the indicator you want to compare."),
h5("Please use the Radio Point to choose which indicator you would like to compare",style = "color:grey"),
sidebarLayout(
sidebarPanel(
radioButtons("indicator", "Choose an Indicator:",
choices = c("dev.norm", "f1", "precision", "recall"),
selected = c("dev.norm")) ,
p('__________________________________'),
h4("Conclusion"),
p("From the observations, the third model (XGBoost Classifier with Variable Combination 2) has a reletively good performance, especially for f1 indicator.
Therefore, we choose this set as the final best model"),
tags$a(href="https://youtu.be/uQKxh1xiHzU?si=vuo7dlFLjMbL8sqK", "Click Here to Watch the Guide Video!")),
mainPanel(
plotOutput("perfPlot")
)
)
),
tabPanel(
"K-Means Clustering",
h3("Clustering Plot (Two-Dimensional Display)", style = "color:Darkred"),
p("This clustering plot shows the result of K-Means Clustering. The plot is a two-dimensional display of the data.
The x-axis is the first principal component, and the y-axis is the second principal component.
The color of the points represents the cluster that the data point belongs to. The number of clusters is determined by the user. "),
h5("Please use the slider choose the number of clusters you want to plot.",style = "color:grey"),
sidebarLayout(
sidebarPanel(
sliderInput("selectn", label="Choose k as clusters number:",
min=2, max=10,value=2) ,
p('__________________________________'),
h4("Conclusion"),
p("From the evaluation method metioned in the report,
the clustering perform best when k=2. When check the k=2,
it shows a reletively clear clusters than k = others. However, there is still a huge overlap between two different clusters,
which indicates it might not a good clustering. The reason might be traced from the amount limitation of obersvations. Therefore, the clustering might not be a good method for this dataset."),
tags$a(href="https://youtu.be/uQKxh1xiHzU?si=vuo7dlFLjMbL8sqK", "Click Here to Watch the Guide Video!")),
mainPanel(
plotOutput("cluster")
)
)
)
)
server <- function(input, output) {
output$ROC <- renderPlot({
plot_roc <- function(predcol, outcol, colour_id=10, overlaid=F){
ROCit_obj <- rocit(score=predcol, class=outcol==pos)
par(new=overlaid)
plot(ROCit_obj,
col = c(colour_id, 10), legend = FALSE, YIndex = FALSE, values = FALSE)
}
if ("region" %in% input$selected_columns)
plot_roc(shiny1$predregion, shiny1[,outcome],colour_id=1)
if ("lst mth view" %in% input$selected_columns)
plot_roc(shiny1$predlast_month_view, shiny1[,outcome], colour_id=2, overlaid=T)
if ("uploads" %in% input$selected_columns)
plot_roc(shiny1$preduploads, shiny1[,outcome],colour_id=3,overlaid=T)
if ("lst mth subscribe" %in% input$selected_columns)
plot_roc(shiny1$predlast_month_subscribe, shiny1[,outcome], colour_id=4, overlaid=T)
if ("subscribers" %in% input$selected_columns)
plot_roc(shiny1$predsubscribers, shiny1[,outcome], colour_id=5, overlaid=T)
if ("views" %in% input$selected_columns)
plot_roc(shiny1$predviews, shiny1[,outcome], colour_id=6, overlaid=T)
if ("Vtreat Missing" %in% input$selected_columns)
plot_roc(shiny1$predsubscribers_for_last_30_days_isBAD, shiny1[,outcome], colour_id=7, overlaid=T)
if ("numeric date" %in% input$selected_columns)
plot_roc(shiny1$prednumeric_date, shiny1[,outcome], colour_id=8, overlaid=T)
if ("category" %in% input$selected_columns)
plot_roc(shiny1$predcategory, shiny1[,outcome], colour_id=13, overlaid=T)
legend("bottomright", legend = input$selected_columns, col = 1:(length(input$selected_columns) + 1), lty = 1)
})
output$cluster <- renderPlot({
k <- input$selectn
groups <- kmeans(scaled_df, k, nstart=100, iter.max=100)$cluster
kmclust.project2D <- cbind(project2D, cluster=as.factor(groups))
kmclust.hull <- find_convex_hull(kmclust.project2D, groups)
ggplot(kmclust.project2D, aes(x=PC1, y=PC2)) + geom_point(aes(shape=cluster, color=cluster)) +
geom_polygon(data=kmclust.hull, aes(group=cluster, fill=cluster), alpha=0.4, linetype=0) +
labs(title = sprintf("Clustering Plot - K = %d", k)) +
scale_x_continuous(name = "Principal Component 1") + scale_y_continuous(name = "Principal Component 2") +
theme(legend.position="bottom", text=element_text(size=15),
plot.title = element_text(hjust = 0.5, size = 19, face = "bold",color="darkred"),
axis.title.x = element_text(size = 13), axis.title.y = element_text(size = 13))
})
output$perfPlot <- renderPlot({
data_to_plot <- all_perf_long[all_perf_long$metric == input$indicator,]
ggplot(data_to_plot, aes(x=model_type, y=value, color=model_type)) +
geom_point(size=8, position=position_dodge(0.8),show.legend = FALSE) +
labs(title="Indicator Performance for 3 Classifiers", y="Indicator Value",x="") +
theme_minimal() +
theme(legend.position="bottom",text=element_text(size=15),
plot.title = element_text(hjust = 0.5, size = 19, face = "bold",color="darkred"),
axis.title.y = element_text(size = 16, face = "bold",color="darkgray"), axis.text.x = element_text(size = 15, face = "bold") ,axis.text.y = element_text(size = 15, face = "bold") )
})
}
shinyApp(ui = ui, server = server)
runApp('~/Project2')
